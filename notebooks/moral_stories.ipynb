{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a64ba63",
   "metadata": {},
   "source": [
    "# Loading the `Moral-Stories` dataset\n",
    "***\n",
    "The dataset and code can be found <a href=\"https://github.com/demelin/moral_stories\">here</a>.\\\n",
    "The authors provide 12k unique norms and, for some reason, additional 700k variations of the same norms, just with NaN fields every now and then. Zero additional information, but maybe I am overlooking something here?\n",
    "* Might be for different tasks? But then they only provide a single label which is always 1 for any NaN rows..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc517ac3",
   "metadata": {},
   "source": [
    "# Sample task: Action classification\n",
    "***\n",
    "For starters, let's reproduce a task from the paper:\n",
    "* Given an action, predict whether it is moral or immoral.\n",
    "* For simplicity, we do not use the splits introduced in the paper, but rather random splitting\n",
    "\n",
    "We start by loading the data as a `pandas.DataFrame`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0bfcfee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ailignment.datasets.moral_stories import get_moral_stories, make_action_classification_dataframe\n",
    "from ailignment.datasets import get_accuracy_metric, join_sentences, tokenize_and_split\n",
    "from transformers import TrainingArguments\n",
    "import pandas as pd\n",
    "import datasets\n",
    "import transformers\n",
    "from ailignment import sequence_classification\n",
    "\n",
    "transformers.logging.set_verbosity_warning()\n",
    "\n",
    "dataframe = get_moral_stories()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "81fbcadc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "# run everything through spacy for part of speech tags\n",
    "dataframe[\"norm_parsed\"] = dataframe[\"norm\"].apply(nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1cd3a074",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pos_until_verb(doc):\n",
    "    '''\n",
    "    Returns a tuple of Part-of-speech tags up until the first verb\n",
    "    Returns (\"EMPTY\") if no VERB is present\n",
    "    '''\n",
    "    l = [d.pos_ for d in doc]\n",
    "    if \"VERB\" not in l: return (\"EMPTY\",)\n",
    "    return tuple(l[:l.index(\"VERB\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aba2e048",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe[\"norm_pos\"] = dataframe[\"norm_parsed\"].apply(lambda x: [d.pos_ for d in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "874dec2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe[\"pos_verb\"] = dataframe[\"norm_parsed\"].apply(get_pos_until_verb)\n",
    "pos_verb = dataframe.groupby(\"pos_verb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "8ec582ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = pos_verb.count().sort_values(\"ID\",ascending=False)\n",
    "#pos_verb.get_group(s.index[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "929f4e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "e = s[\"ID\"].cumsum()/s[\"ID\"].sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "d211ace4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>norm</th>\n",
       "      <th>situation</th>\n",
       "      <th>intention</th>\n",
       "      <th>moral_action</th>\n",
       "      <th>moral_consequence</th>\n",
       "      <th>immoral_action</th>\n",
       "      <th>immoral_consequence</th>\n",
       "      <th>norm_parsed</th>\n",
       "      <th>norm_pos</th>\n",
       "      <th>pos_verb</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7222</th>\n",
       "      <td>3TVRFO09GLDUXBWS12SEEPJ2264LXJ</td>\n",
       "      <td>If your job is preparing food you should wear ...</td>\n",
       "      <td>Sally works in the kitchen at a diner and is c...</td>\n",
       "      <td>Sally wants to create a meal.</td>\n",
       "      <td>Sally cuts the chicken wearing gloves and afte...</td>\n",
       "      <td>Sally is able to make a healthy meal that the ...</td>\n",
       "      <td>Sally holds the chicken with her bare hand and...</td>\n",
       "      <td>Sally cross contaminates the vegetables with c...</td>\n",
       "      <td>(If, your, job, is, preparing, food, you, shou...</td>\n",
       "      <td>[SCONJ, PRON, NOUN, AUX, VERB, NOUN, PRON, AUX...</td>\n",
       "      <td>(SCONJ, PRON, NOUN, AUX)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  ID  \\\n",
       "7222  3TVRFO09GLDUXBWS12SEEPJ2264LXJ   \n",
       "\n",
       "                                                   norm  \\\n",
       "7222  If your job is preparing food you should wear ...   \n",
       "\n",
       "                                              situation  \\\n",
       "7222  Sally works in the kitchen at a diner and is c...   \n",
       "\n",
       "                          intention  \\\n",
       "7222  Sally wants to create a meal.   \n",
       "\n",
       "                                           moral_action  \\\n",
       "7222  Sally cuts the chicken wearing gloves and afte...   \n",
       "\n",
       "                                      moral_consequence  \\\n",
       "7222  Sally is able to make a healthy meal that the ...   \n",
       "\n",
       "                                         immoral_action  \\\n",
       "7222  Sally holds the chicken with her bare hand and...   \n",
       "\n",
       "                                    immoral_consequence  \\\n",
       "7222  Sally cross contaminates the vegetables with c...   \n",
       "\n",
       "                                            norm_parsed  \\\n",
       "7222  (If, your, job, is, preparing, food, you, shou...   \n",
       "\n",
       "                                               norm_pos  \\\n",
       "7222  [SCONJ, PRON, NOUN, AUX, VERB, NOUN, PRON, AUX...   \n",
       "\n",
       "                      pos_verb  \n",
       "7222  (SCONJ, PRON, NOUN, AUX)  "
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_verb.get_group(s.index[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "590559e7",
   "metadata": {},
   "source": [
    "# Untying the *is* from the *ought*\n",
    "***\n",
    "We are not to come up with new norms. Rather, we like to get rid of the usually ambiguous normative judgements. Hence, we aim to split the moral value assigned to a norm from the situation it applies to. Checking for norm violation then reduces to other well-known problems such as question answering or textual entailment.\n",
    "\n",
    "**TODO:** Surely there are smart names for the two categories of *situations that are judge-worthy* and their corresponding moral values.\n",
    "\n",
    "We look for the most common ways that norms are phrased:\n",
    "* \"You should\" or \"You shouldn't\"\n",
    "* \"It's\" or \"It is\" followed by good, bad, etc.\n",
    "\n",
    "Then, we want to translate them into \"Did you ....?\" and \"Well, that's ...\" pieces\n",
    "* E.g. \"You should back up so you can let people park.\" becomes\n",
    "    * \"Did you backup so you can let people park?\"\n",
    "    * \"Yes\" -> \"Good\"\n",
    "    * \"No\" -> \"Well, you should have!\"\n",
    "\n",
    "The norms only give value for either violation or adherence, but rarely both. We mirror the sentiment of the judgement by simple negation:\n",
    "* \"It is bad to hurt people\"\n",
    "    * \"Did you hurt people?\"\n",
    "    * \"Yes\" --> \"bad\"\n",
    "    * \"No\" --> \"not bad\"\n",
    "\n",
    "Note, that we refrain from translating \"not bad\" to \"good\", since norms often aren't morally symmetric, i.e. not every norm has equally strong normativity for violation and adherence. E.g. positively committing a crime is punished much more explicitly than is not committing it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "0a418f30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"You should\": 741\n",
      "\"You shouldnt\": 2302\n",
      "\"It's\" 8087\n",
      "Covered 11130 of 11999 (92.76%) norms\n"
     ]
    }
   ],
   "source": [
    "norms = dataframe[\"norm\"].apply(str.lower)\n",
    "\n",
    "f_should = lambda x: x.startswith(\"you should \") and not x.startswith(\"you should not\")\n",
    "shoulds = dataframe[norms.apply(f_should)]\n",
    "print(\"\\\"You should\\\":\", len(shoulds))\n",
    "\n",
    "f_shouldnt = lambda x: x.startswith(\"you shouldn\") or x.startswith(\"you should not\")\n",
    "shouldnts = dataframe[norms.apply(f_shouldnt)]\n",
    "print(\"\\\"You shouldnt\\\":\", len(shouldnts))\n",
    "\n",
    "f_its = lambda x: x.startswith(\"it's\") or x.startswith(\"it is\")\n",
    "its = dataframe[norms.apply(f_its)]\n",
    "print(\"\\\"It's\\\"\", len(its))\n",
    "\n",
    "total = len(its)+len(shoulds)+len(shouldnts)\n",
    "print(\"Covered\", total ,\"of\",len(dataframe), f\"({100*total/len(dataframe):.2f}%) norms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "786cc4bf",
   "metadata": {},
   "source": [
    "### Untying `you should` sentences\n",
    "***\n",
    "First, find the most common grammatical structures:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "75767217",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "norm_pos\n",
       "(PRON, AUX, VERB)    564\n",
       "(PRON, AUX, ADV)     173\n",
       "(PRON, AUX, AUX)       3\n",
       "(PRON, AUX, PRON)      1\n",
       "Name: ID, dtype: int64"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "common_pos = shoulds[\"norm_pos\"].apply(lambda x: tuple(x[:3]))\n",
    "common_pos = shoulds.groupby(common_pos)\n",
    "most_common = common_pos[\"ID\"].count().sort_values(ascending=False)\n",
    "most_common"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54e82c65",
   "metadata": {},
   "source": [
    "Only two seem to be of relevance:\n",
    "* \"You should VERB\"\n",
    "    * You should help the ones in need\n",
    "* \"You should ADV\"\n",
    "    * You should always, You should never\n",
    "\n",
    "While the first is straightforward to de-value, the latter at least requires more sophisticated reasoning capabilities concerning quantification and negation of cases.\n",
    "\n",
    "For now, we simply strip the initial \"You should\" from the former, and the \"You should ADV\" from the latter to obtain the judgements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "deda2a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# do first group of (PRON, AUX, VERB)\n",
    "g0 = common_pos.get_group((\"PRON\",\"AUX\",\"VERB\")).copy()\n",
    "g0[\"norm_devalued\"] = g0[\"norm_parsed\"].apply(lambda x: x[2:].text)\n",
    "g0[\"value\"] = \"You should\"\n",
    "\n",
    "# second group of (PRON, AUX, ADV)\n",
    "g1 = common_pos.get_group((\"PRON\",\"AUX\",\"ADV\")).copy()\n",
    "g1[\"norm_devalued\"] = g1[\"norm_parsed\"].apply(lambda x: x[3:].text)\n",
    "g1[\"value\"] = g1[\"norm_parsed\"].apply(lambda x: \"You should \" + x[2].text)\n",
    "\n",
    "shoulds_devalued = pd.concat([g0,g1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d83e4aee",
   "metadata": {},
   "source": [
    "### Untying `you should not` sentences\n",
    "***\n",
    "First, find the most common grammatical structures:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "d4f64310",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "norm_pos\n",
       "(PRON, AUX, PART, VERB)     2236\n",
       "(PRON, AUX, PART, ADV)        35\n",
       "(PRON, AUX, PART, AUX)        19\n",
       "(PRON, AUX, PART, ADJ)         3\n",
       "(PRON, AUX, PART, ADP)         3\n",
       "(PRON, AUX, PART, NOUN)        3\n",
       "(PRON, AUX, PART, PART)        1\n",
       "(PRON, AUX, PART, PUNCT)       1\n",
       "(PRON, AUX, VERB, NOUN)        1\n",
       "Name: ID, dtype: int64"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "common_pos = shouldnts[\"norm_pos\"].apply(lambda x: tuple(x[:4]))\n",
    "common_pos = shouldnts.groupby(common_pos)\n",
    "most_common = common_pos[\"ID\"].count().sort_values(ascending=False)\n",
    "most_common"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baf23927",
   "metadata": {},
   "source": [
    "Only one seems to be of relevance:\n",
    "* \"You should not VERB\"\n",
    "    * You should not spit in someone's face\n",
    "\n",
    "Similarly to the \"You should\" cases, we strip \"You should not\" as the value-judgement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "f9f53e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "shouldnts_devalued = common_pos.get_group((\"PRON\",\"AUX\",\"PART\", \"VERB\")).copy()\n",
    "shouldnts_devalued[\"norm_devalued\"] = shouldnts_devalued[\"norm_parsed\"].apply(lambda x: x[3:].text)\n",
    "shouldnts_devalued[\"value\"] = \"You should not\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f42784c",
   "metadata": {},
   "source": [
    "### Untying `it is` sentences\n",
    "***\n",
    "First, find the most common grammatical structures up until the first verb:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "53ffab95",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pos_verb\n",
       "(PRON, AUX, ADJ, PART)                    6518\n",
       "(PRON, AUX, PART, ADJ, PART)               373\n",
       "(PRON, AUX, ADJ, PART, PART)               344\n",
       "(PRON, AUX)                                236\n",
       "(PRON, AUX, ADJ, PART, ADV)                105\n",
       "(EMPTY,)                                   101\n",
       "(PRON, AUX, ADJ, PART, AUX)                 73\n",
       "(PRON, AUX, ADJ, ADP, NOUN, PART)           49\n",
       "(PRON, AUX, ADV, ADJ, PART)                 35\n",
       "(PRON, AUX, PART, ADJ, PART, PART)          20\n",
       "(PRON, AUX, ADJ, ADV, NOUN)                 16\n",
       "(PRON, AUX, ADJ, ADP)                       14\n",
       "(PRON, AUX, ADJ, ADP, DET, NOUN, PART)      13\n",
       "(PRON, AUX, NOUN, PART)                     10\n",
       "(PRON, AUX, ADJ, NOUN, PART)                 9\n",
       "(PRON, AUX, PART)                            9\n",
       "(PRON, AUX, PART, ADJ, PART, AUX)            8\n",
       "(PRON, AUX, ADJ)                             6\n",
       "(PRON, AUX, ADJ, ADP, PRON, PART)            5\n",
       "(PRON, AUX, ADJ, CCONJ, ADJ, PART)           5\n",
       "Name: ID, dtype: int64"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#common_pos = its[\"norm_pos\"].apply(lambda x: tuple(x[:5]))\n",
    "common_pos = its.groupby(\"pos_verb\")\n",
    "most_common = common_pos[\"ID\"].count().sort_values(ascending=False)\n",
    "most_common[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c92ac576",
   "metadata": {},
   "source": [
    "The situation is a little more complex here, it seems:\n",
    "1. \"It is ADJ to\", It's wrong to become addicted to gambling.\n",
    "    * Value: ADJ, strip until first verb\n",
    "2. \"It is not ADJ to\", It's not okay to invade someone else's privacy.\n",
    "    * Like 1.\n",
    "3. (PRON, AUX, ADJ, PART, PART). Two subgroups, which are both currently ignored\n",
    "    1. \"It is ADJ not to\"\n",
    "    2. \"It is ADJ to not\"\n",
    "\n",
    "Similarly to the \"You should\" cases, we strip \"You should not\" as the value-judgement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "3d5e5fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "groups = []\n",
    "# (PRON, AUX, ADJ, PART)\n",
    "g = common_pos.get_group((\"PRON\",\"AUX\",\"ADJ\", \"PART\")).copy()\n",
    "g[\"norm_devalued\"] = g[\"norm_parsed\"].apply(lambda x: x[4:].text)\n",
    "g[\"value\"] = g[\"norm_parsed\"].apply(lambda x: x[2].text)\n",
    "groups.append(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "c86d78ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (PRON, AUX, PART, ADJ, PART)\n",
    "g = common_pos.get_group((\"PRON\",\"AUX\",\"PART\",\"ADJ\", \"PART\")).copy()\n",
    "g[\"norm_devalued\"] = g[\"norm_parsed\"].apply(lambda x: x[5:].text)\n",
    "g[\"value\"] = g[\"norm_parsed\"].apply(lambda x: x[2:4].text)\n",
    "groups.append(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "549b8a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (PRON, AUX, ADJ, PART, PART)\n",
    "g = common_pos.get_group((\"PRON\",\"AUX\",\"ADJ\",\"PART\", \"PART\")).copy()\n",
    "g[\"norm_devalued\"] = g[\"norm_parsed\"].apply(lambda x: x[5:].text)\n",
    "g[\"value\"] = g[\"norm_parsed\"].apply(lambda x: x[2:4].text)\n",
    "# currently ignored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "f2d7d13d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (PRON, AUX)\n",
    "g = common_pos.get_group((\"PRON\",\"AUX\")).copy()\n",
    "g[\"norm_devalued\"] = g[\"norm_parsed\"].apply(lambda x: x[5:].text)\n",
    "g[\"value\"] = g[\"norm_parsed\"].apply(lambda x: x[2:4].text)\n",
    "# currently ignored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "20608364",
   "metadata": {},
   "outputs": [],
   "source": [
    "# finally, stitch everything together\n",
    "its_devalued = pd.concat(groups)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e19b2b7d",
   "metadata": {},
   "source": [
    "# Imperatives and values\n",
    "***\n",
    "By splitting up value-judgements from the norms we end up with imperatives describing what the action looks like and the value one should expect to receive from it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "428df472",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After devaluation, we cover 9864 of 11999 (82.21%) norms\n"
     ]
    }
   ],
   "source": [
    "dataframe_devalued = pd.concat([shoulds_devalued, shouldnts_devalued, its_devalued])\n",
    "total_split = len(dataframe_devalued)\n",
    "print(\"After devaluation, we cover\", total_split ,\"of\",len(dataframe), f\"({100*total_split/len(dataframe):.2f}%) norms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "cecc1784",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>moral_action</th>\n",
       "      <th>norm_devalued</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5285</th>\n",
       "      <td>Douglas picks up the new game and purchased it...</td>\n",
       "      <td>steal things from a business.</td>\n",
       "      <td>wrong</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8933</th>\n",
       "      <td>Anders talks to his primary care doctor about ...</td>\n",
       "      <td>take Xanax if it wasn't prescribed to you.</td>\n",
       "      <td>You should not</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10629</th>\n",
       "      <td>John found a restroom in a shop where he was b...</td>\n",
       "      <td>expose yourself to strangers.</td>\n",
       "      <td>You should not</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6605</th>\n",
       "      <td>Martin decides to tackle some really difficult...</td>\n",
       "      <td>drink yourself to excess.</td>\n",
       "      <td>not okay</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4047</th>\n",
       "      <td>Matt takes a seat at table with some friendly ...</td>\n",
       "      <td>eat with other people.</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6624</th>\n",
       "      <td>Tom deescalates the situation by apologizing a...</td>\n",
       "      <td>shoot arrows at people.</td>\n",
       "      <td>bad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2395</th>\n",
       "      <td>Jonny tells his friends to start the movie wit...</td>\n",
       "      <td>expect people to wait for you.</td>\n",
       "      <td>You should not</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6815</th>\n",
       "      <td>Jim decides to leave the kids alone and let th...</td>\n",
       "      <td>try and ruin someone else good time.</td>\n",
       "      <td>wrong</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1969</th>\n",
       "      <td>John avoids saying anything and cautiously dri...</td>\n",
       "      <td>yell at an elder.</td>\n",
       "      <td>rude</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8633</th>\n",
       "      <td>John tells his son that these things happen an...</td>\n",
       "      <td>get mad at your family.</td>\n",
       "      <td>You should not</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            moral_action  \\\n",
       "5285   Douglas picks up the new game and purchased it...   \n",
       "8933   Anders talks to his primary care doctor about ...   \n",
       "10629  John found a restroom in a shop where he was b...   \n",
       "6605   Martin decides to tackle some really difficult...   \n",
       "4047   Matt takes a seat at table with some friendly ...   \n",
       "6624   Tom deescalates the situation by apologizing a...   \n",
       "2395   Jonny tells his friends to start the movie wit...   \n",
       "6815   Jim decides to leave the kids alone and let th...   \n",
       "1969   John avoids saying anything and cautiously dri...   \n",
       "8633   John tells his son that these things happen an...   \n",
       "\n",
       "                                    norm_devalued           value  \n",
       "5285                steal things from a business.           wrong  \n",
       "8933   take Xanax if it wasn't prescribed to you.  You should not  \n",
       "10629               expose yourself to strangers.  You should not  \n",
       "6605                    drink yourself to excess.        not okay  \n",
       "4047                       eat with other people.            good  \n",
       "6624                      shoot arrows at people.             bad  \n",
       "2395               expect people to wait for you.  You should not  \n",
       "6815         try and ruin someone else good time.           wrong  \n",
       "1969                            yell at an elder.            rude  \n",
       "8633                      get mad at your family.  You should not  "
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe_devalued[[\"moral_action\",\"norm_devalued\",\"value\"]].sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f6592a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for wd in [0,0.01,0.05,0.1]:\n",
    "    for lr in [1e-5, 5e-5]:\n",
    "        test_split = 0.2\n",
    "        batch_size = 12\n",
    "        model = \"distilbert-base-uncased\"\n",
    "        #model = \"albert-base-v2\"\n",
    "        action_dataframe = make_action_classification_dataframe(dataframe)\n",
    "        input_columns = [\"action\"]\n",
    "        action_dataframe[\"task_input\"] = join_sentences(action_dataframe, input_columns, \" \")\n",
    "        dataset = datasets.Dataset.from_pandas(action_dataframe)\n",
    "        dataset = dataset.train_test_split(test_size=test_split)\n",
    "\n",
    "        def data_all(tokenizer):\n",
    "            return tokenize_and_split(dataset, tokenizer, \"task_input\")\n",
    "        def data_small(tokenizer):\n",
    "            train, test = data_all(tokenizer)\n",
    "            train = train.shuffle(seed=42).select(range(1000))\n",
    "            test = test.shuffle(seed=42).select(range(1000))\n",
    "            return train, test\n",
    "\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=\"results/\",\n",
    "            num_train_epochs=3,\n",
    "            per_device_train_batch_size=batch_size,\n",
    "            per_device_eval_batch_size=batch_size,\n",
    "            warmup_steps=500,\n",
    "            weight_decay=wd,\n",
    "            learning_rate=lr,\n",
    "            logging_dir='./logs',\n",
    "            logging_steps=500,\n",
    "            save_steps=1000000,\n",
    "            save_total_limit=0,\n",
    "            evaluation_strategy=\"epoch\",\n",
    "        )\n",
    "\n",
    "        r = sequence_classification(data_all, model, training_args, get_accuracy_metric())\n",
    "        acc = [x[\"eval_accuracy\"] for x in r if \"eval_accuracy\" in x]\n",
    "        results.append(r)\n",
    "        print(wd, lr, acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "157df0bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "[[x[\"eval_accuracy\"] for x in r if \"eval_accuracy\" in x] for r in results]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ce840a5",
   "metadata": {},
   "source": [
    "# WIP: Get score output from LM\n",
    "***\n",
    "Question: Is there a better way to sample from generated LM outputs?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e617675",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    AutoModelForSequenceClassification, DistilBertTokenizerFast,\n",
    "     Trainer, TrainingArguments, AutoModelWithLMHead, AutoTokenizer,\n",
    ")\n",
    "import torch\n",
    "\n",
    "model = \"distilbert-base-uncased\"\n",
    "model = \"gpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "model = AutoModelWithLMHead.from_pretrained(model)\n",
    "\n",
    "prompt = \"Today the weather is really nice and I am planning on \"\n",
    "inputs = tokenizer.encode(prompt, add_special_tokens=False, return_tensors=\"pt\")\n",
    "\n",
    "prompt_length = len(tokenizer.decode(inputs[0], skip_special_tokens=True, clean_up_tokenization_spaces=True))\n",
    "outputs = model.generate(inputs, max_length=250, do_sample=False, top_p=0.95, top_k=60,\n",
    "                        return_dict_in_generate=True, output_attentions=False,\n",
    "                        output_hidden_states=True, output_scores=True)\n",
    "#generated = prompt + tokenizer.decode(outputs[0])[prompt_length:]\n",
    "\n",
    "p = torch.softmax(outputs.scores[0], dim=1)\n",
    "\n",
    "print(p.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d87a84ed",
   "metadata": {},
   "source": [
    "# WIP: Data augmentation with NER\n",
    "***\n",
    "Idea: Use Named entity recognition to find and replace persons etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04c818fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ailignment.datasets.moral_stories import get_moral_stories, make_action_classification_dataframe\n",
    "from ailignment import join_sentences, tokenize_and_split, get_accuracy_metric\n",
    "dataframe = get_moral_stories()\n",
    "columns = dataframe.columns[1:]\n",
    "print(\"Running NER on columns\", columns.to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4b146a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = join_sentences(dataframe ,columns, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d4beffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "ner_pipe = nlp.pipe(tqdm(texts), disable=['tagger', 'parser', 'attribute_ruler', 'lemmatizer'])\n",
    "docs = [x for x in ner_pipe]\n",
    "\n",
    "displacy.render(docs[0], style=\"ent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b61607f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def get_frequent_entity(doc, entity=\"PERSON\", n=1):\n",
    "    '''\n",
    "    Returns the highest number of occurences of an\n",
    "    entity in the NER doc.\n",
    "    '''\n",
    "    occurences = [(x.text, x.label_) for x in doc.ents if x.label_ == entity]\n",
    "    c = Counter(occurences)\n",
    "    ents = []\n",
    "    for item, count in c.most_common(n):\n",
    "        ents.append([x for x in doc.ents if (x.text, x.label_) == item])\n",
    "    \n",
    "    if n == 1 and len(ents) != 0:\n",
    "        ents = ents[0]\n",
    "    return ents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "965139d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "persons = [get_frequent_entity(x, \"PERSON\",n=1) for x in docs]\n",
    "# we are interested in the simplest case, where the NER found\n",
    "# exactly 6 matches\n",
    "matches = [x for x in persons if len(x) == 6]\n",
    "print(f\"Found {len(matches)} matches\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56082bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = matches[0]\n",
    "displacy.render(m[0].doc, \"ent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ded8d9e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_entity(ents, s):\n",
    "    '''\n",
    "    Replaces all occurences of entities in `ents` with `s`.\n",
    "    `ents` is a list of entities as returned by `doc.ents`\n",
    "    from an NER pipeline, they need to be from the same doc!\n",
    "    '''\n",
    "    offset = 0\n",
    "    text = ents[0].doc.text\n",
    "    new_text = \"\"\n",
    "    for ent in ents:\n",
    "        start = ent.start_char\n",
    "        end = ent.end_char\n",
    "        left = text[offset:start]\n",
    "        new_text += left + s\n",
    "        offset = end\n",
    "    new_text += text[offset:]\n",
    "    return new_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aae19b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_docs = [replace_entity(m, \"Niklas\").split(\"\\n\") for m in matches]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beee911a",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_docs = [m[0].doc.text.split(\"\\n\") for m in matches]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "863d1fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe_replaced = pd.DataFrame(n_docs)\n",
    "dataframe_replaced.columns = columns\n",
    "dataframe_replaced.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e1896b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "test_split = 0.2\n",
    "batch_size = 8\n",
    "\n",
    "action_dataframe = make_action_classification_dataframe(dataframe_replaced)\n",
    "\n",
    "input_columns = [\"action\"]\n",
    "action_dataframe[\"task_input\"] = join_sentences(action_dataframe, input_columns, \" \")\n",
    "dataset = datasets.Dataset.from_pandas(action_dataframe)\n",
    "dataset = dataset.train_test_split(test_size=test_split)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b5b78c1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "from transformers import (\n",
    "    AutoModelForSequenceClassification, DistilBertTokenizerFast,\n",
    "     Trainer, TrainingArguments, AutoModelWithLMHead, AutoTokenizer,\n",
    ")\n",
    "import torch\n",
    "\n",
    "model = \"distilbert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model)\n",
    "\n",
    "train_data, test_data = tokenize_and_split(dataset, tokenizer, \"task_input\")\n",
    "\n",
    "# for prototyping, optional\n",
    "small_train_data = train_data.shuffle(seed=42).select(range(1000))\n",
    "small_test_data = test_data.shuffle(seed=42).select(range(1000))\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"results/\",\n",
    "    num_train_epochs=5,              # total number of training epochs\n",
    "    per_device_train_batch_size=8,  # batch size per device during training\n",
    "    per_device_eval_batch_size=8,   # batch size for evaluation\n",
    "    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01,               # strength of weight decay\n",
    "    logging_dir='./logs',            # directory for storing logs\n",
    "    logging_steps=50,                # how often to log\n",
    "    save_steps=1000,\n",
    "    save_total_limit=0,\n",
    "    evaluation_strategy=\"epoch\",     # when to run evaluation\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e22e0723",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,                         # the instantiated ðŸ¤— Transformers model to be trained\n",
    "    args=training_args,                  # training arguments, defined above\n",
    "    train_dataset=small_train_data,   # training dataset\n",
    "    eval_dataset=small_test_data,     # evaluation dataset\n",
    "    compute_metrics=get_accuracy_metric,     # code to run accuracy metric\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ebbeaab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1711b948",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gender_guesser.detector import Detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12fd8975",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = Detector()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6774cae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "d.get_gender(\"Jamie\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ff68cec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ailignment.datasets.moral_stories import _lemmatize, get_moral_stories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2e4a88fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\kiehne\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from string import punctuation\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2fa31496",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm', disable=['parser', 'textcat'])\n",
    "STOP_WORDS = stopwords.words('english')\n",
    "stories = get_moral_stories()\n",
    "columns = [\"moral_action\", \"immoral_action\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9b4c2f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "series = stories[columns[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "29b71164",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_series(series, nlp, STOP_WORDS=None):\n",
    "    '''\n",
    "    Given a series of strings, returns a DataFrame([\"lemmas\", \"tokens\", \"maps\"])\n",
    "    of the lemmatized strings according to `_lemmatize` function.\n",
    "    '''\n",
    "    # get rid of whitespace\n",
    "    translation_table = str.maketrans(' ', ' ', punctuation)\n",
    "    series = series.map(lambda x: x.translate(translation_table))\n",
    "    series = series.map(lambda x: _lemmatize(x, nlp, STOP_WORDS))\n",
    "    data = pd.DataFrame(series.to_list(), columns=[\"lemmas\", \"tokens\", \"maps\"])\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "35699cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = series.map(lambda x: _lemmatize(x, nlp, STOP_WORDS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a3a3585e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame(y.to_list(), columns=[\"lemmas\", \"tokens\", \"maps\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3583c702",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lemmas</th>\n",
       "      <th>tokens</th>\n",
       "      <th>maps</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[kevin, decide, &lt;STPWRD&gt;, place, &lt;STPWRD&gt;, wag...</td>\n",
       "      <td>[Kevin, decides, to, place, a, wager, once, ev...</td>\n",
       "      <td>{0: [0], 1: [1], 2: [2], 3: [3], 4: [4], 5: [5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[curtis, explain, &lt;STPWRD&gt;, &lt;STPWRD&gt;, woman, &lt;...</td>\n",
       "      <td>[Curtis, explains, to, the, woman, that, he's,...</td>\n",
       "      <td>{0: [0], 1: [1], 2: [2], 3: [3], 4: [4], 5: [5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[anna, decide, &lt;STPWRD&gt;, attend, &lt;STPWRD&gt;, wed...</td>\n",
       "      <td>[Anna, decides, to, attend, the, wedding, even...</td>\n",
       "      <td>{0: [0], 1: [1], 2: [2], 3: [3], 4: [4], 5: [5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[greg, tell, james, &lt;STPWRD&gt;, &lt;STPWRD&gt;, &lt;STPWR...</td>\n",
       "      <td>[Greg, tells, James, that, he, can, play, tomo...</td>\n",
       "      <td>{0: [0], 1: [1], 2: [2], 3: [3], 4: [4], 5: [5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[ben, call, &lt;STPWRD&gt;, tow, truck, &lt;STPWRD&gt;, ge...</td>\n",
       "      <td>[Ben, calls, a, tow, truck, and, gets, the, ca...</td>\n",
       "      <td>{0: [0], 1: [1], 2: [2], 3: [3], 4: [4], 5: [5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11994</th>\n",
       "      <td>[payton, ask, &lt;STPWRD&gt;, friend, &lt;STPWRD&gt;, &lt;STP...</td>\n",
       "      <td>[Payton, asks, her, friend, where, she, bought...</td>\n",
       "      <td>{0: [0], 1: [1], 2: [2], 3: [3], 4: [4], 5: [5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11995</th>\n",
       "      <td>[helen, &lt;STPWRD&gt;, &lt;STPWRD&gt;, conversation, &lt;STP...</td>\n",
       "      <td>[Helen, has, a, conversation, with, her, husba...</td>\n",
       "      <td>{0: [0], 1: [1], 2: [2], 3: [3], 4: [4], 5: [5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11996</th>\n",
       "      <td>[cole, move, away, &lt;STPWRD&gt;, &lt;STPWRD&gt;, vehicle...</td>\n",
       "      <td>[Cole, moves, away, from, the, vehicle, that, ...</td>\n",
       "      <td>{0: [0], 1: [1], 2: [2], 3: [3], 4: [4], 5: [5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11997</th>\n",
       "      <td>[harry, ask, steve, &lt;STPWRD&gt;, &lt;STPWRD&gt;, &lt;STPWR...</td>\n",
       "      <td>[Harry, asks, Steve, what, he, should, be, doi...</td>\n",
       "      <td>{0: [0], 1: [1], 2: [2], 3: [3], 4: [4], 5: [5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11998</th>\n",
       "      <td>[andrew, confront, &lt;STPWRD&gt;, father, ,, &lt;STPWR...</td>\n",
       "      <td>[Andrew, confronts, his, father,, and, tells, ...</td>\n",
       "      <td>{0: [0], 1: [1], 2: [2], 3: [3], 4: [3], 5: [4...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11999 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  lemmas  \\\n",
       "0      [kevin, decide, <STPWRD>, place, <STPWRD>, wag...   \n",
       "1      [curtis, explain, <STPWRD>, <STPWRD>, woman, <...   \n",
       "2      [anna, decide, <STPWRD>, attend, <STPWRD>, wed...   \n",
       "3      [greg, tell, james, <STPWRD>, <STPWRD>, <STPWR...   \n",
       "4      [ben, call, <STPWRD>, tow, truck, <STPWRD>, ge...   \n",
       "...                                                  ...   \n",
       "11994  [payton, ask, <STPWRD>, friend, <STPWRD>, <STP...   \n",
       "11995  [helen, <STPWRD>, <STPWRD>, conversation, <STP...   \n",
       "11996  [cole, move, away, <STPWRD>, <STPWRD>, vehicle...   \n",
       "11997  [harry, ask, steve, <STPWRD>, <STPWRD>, <STPWR...   \n",
       "11998  [andrew, confront, <STPWRD>, father, ,, <STPWR...   \n",
       "\n",
       "                                                  tokens  \\\n",
       "0      [Kevin, decides, to, place, a, wager, once, ev...   \n",
       "1      [Curtis, explains, to, the, woman, that, he's,...   \n",
       "2      [Anna, decides, to, attend, the, wedding, even...   \n",
       "3      [Greg, tells, James, that, he, can, play, tomo...   \n",
       "4      [Ben, calls, a, tow, truck, and, gets, the, ca...   \n",
       "...                                                  ...   \n",
       "11994  [Payton, asks, her, friend, where, she, bought...   \n",
       "11995  [Helen, has, a, conversation, with, her, husba...   \n",
       "11996  [Cole, moves, away, from, the, vehicle, that, ...   \n",
       "11997  [Harry, asks, Steve, what, he, should, be, doi...   \n",
       "11998  [Andrew, confronts, his, father,, and, tells, ...   \n",
       "\n",
       "                                                    maps  \n",
       "0      {0: [0], 1: [1], 2: [2], 3: [3], 4: [4], 5: [5...  \n",
       "1      {0: [0], 1: [1], 2: [2], 3: [3], 4: [4], 5: [5...  \n",
       "2      {0: [0], 1: [1], 2: [2], 3: [3], 4: [4], 5: [5...  \n",
       "3      {0: [0], 1: [1], 2: [2], 3: [3], 4: [4], 5: [5...  \n",
       "4      {0: [0], 1: [1], 2: [2], 3: [3], 4: [4], 5: [5...  \n",
       "...                                                  ...  \n",
       "11994  {0: [0], 1: [1], 2: [2], 3: [3], 4: [4], 5: [5...  \n",
       "11995  {0: [0], 1: [1], 2: [2], 3: [3], 4: [4], 5: [5...  \n",
       "11996  {0: [0], 1: [1], 2: [2], 3: [3], 4: [4], 5: [5...  \n",
       "11997  {0: [0], 1: [1], 2: [2], 3: [3], 4: [4], 5: [5...  \n",
       "11998  {0: [0], 1: [1], 2: [2], 3: [3], 4: [3], 5: [4...  \n",
       "\n",
       "[11999 rows x 3 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "017f0c5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a3553355",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfbf5d72",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
