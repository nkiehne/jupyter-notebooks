{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a64ba63",
   "metadata": {},
   "source": [
    "# Loading the `Moral-Stories` dataset\n",
    "***\n",
    "The dataset and code can be found <a href=\"https://github.com/demelin/moral_stories\">here</a>.\\\n",
    "The authors provide 12k unique norms and, for some reason, additional 700k variations of the same norms, just with NaN fields every now and then. Zero additional information, but maybe I am overlooking something here?\n",
    "* Might be for different tasks? But then they only provide a single label which is always 1 for any NaN rows..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc517ac3",
   "metadata": {},
   "source": [
    "# Sample task: Action classification\n",
    "***\n",
    "For starters, let's reproduce a task from the paper:\n",
    "* Given an action, predict whether it is moral or immoral.\n",
    "* For simplicity, we do not use the splits introduced in the paper, but rather random splitting\n",
    "\n",
    "We start by loading the data as a `pandas.DataFrame`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bfcfee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ailignment.datasets.moral_stories import get_moral_stories, make_action_classification_dataframe\n",
    "from ailignment.datasets import get_accuracy_metric, join_sentences, tokenize_and_split\n",
    "from transformers import TrainingArguments\n",
    "import pandas as pd\n",
    "import datasets\n",
    "import transformers\n",
    "from ailignment import sequence_classification\n",
    "\n",
    "transformers.logging.set_verbosity_warning()\n",
    "\n",
    "dataframe = get_moral_stories()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f6592a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for wd in [0,0.01,0.05,0.1]:\n",
    "    for lr in [1e-5, 5e-5]:\n",
    "        test_split = 0.2\n",
    "        batch_size = 12\n",
    "        model = \"distilbert-base-uncased\"\n",
    "        #model = \"albert-base-v2\"\n",
    "        action_dataframe = make_action_classification_dataframe(dataframe)\n",
    "        input_columns = [\"action\"]\n",
    "        action_dataframe[\"task_input\"] = join_sentences(action_dataframe, input_columns, \" \")\n",
    "        dataset = datasets.Dataset.from_pandas(action_dataframe)\n",
    "        dataset = dataset.train_test_split(test_size=test_split)\n",
    "\n",
    "        def data_all(tokenizer):\n",
    "            return tokenize_and_split(dataset, tokenizer, \"task_input\")\n",
    "        def data_small(tokenizer):\n",
    "            train, test = data_all(tokenizer)\n",
    "            train = train.shuffle(seed=42).select(range(1000))\n",
    "            test = test.shuffle(seed=42).select(range(1000))\n",
    "            return train, test\n",
    "\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=\"results/\",\n",
    "            num_train_epochs=3,\n",
    "            per_device_train_batch_size=batch_size,\n",
    "            per_device_eval_batch_size=batch_size,\n",
    "            warmup_steps=500,\n",
    "            weight_decay=wd,\n",
    "            learning_rate=lr,\n",
    "            logging_dir='./logs',\n",
    "            logging_steps=500,\n",
    "            save_steps=1000000,\n",
    "            save_total_limit=0,\n",
    "            evaluation_strategy=\"epoch\",\n",
    "        )\n",
    "\n",
    "        r = sequence_classification(data_all, model, training_args, get_accuracy_metric())\n",
    "        acc = [x[\"eval_accuracy\"] for x in r if \"eval_accuracy\" in x]\n",
    "        results.append(r)\n",
    "        print(wd, lr, acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "157df0bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "[[x[\"eval_accuracy\"] for x in r if \"eval_accuracy\" in x] for r in results]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ce840a5",
   "metadata": {},
   "source": [
    "# WIP: Get score output from LM\n",
    "***\n",
    "Question: Is there a better way to sample from generated LM outputs?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e617675",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    AutoModelForSequenceClassification, DistilBertTokenizerFast,\n",
    "     Trainer, TrainingArguments, AutoModelWithLMHead, AutoTokenizer,\n",
    ")\n",
    "import torch\n",
    "\n",
    "model = \"distilbert-base-uncased\"\n",
    "model = \"gpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "model = AutoModelWithLMHead.from_pretrained(model)\n",
    "\n",
    "prompt = \"Today the weather is really nice and I am planning on \"\n",
    "inputs = tokenizer.encode(prompt, add_special_tokens=False, return_tensors=\"pt\")\n",
    "\n",
    "prompt_length = len(tokenizer.decode(inputs[0], skip_special_tokens=True, clean_up_tokenization_spaces=True))\n",
    "outputs = model.generate(inputs, max_length=250, do_sample=False, top_p=0.95, top_k=60,\n",
    "                        return_dict_in_generate=True, output_attentions=False,\n",
    "                        output_hidden_states=True, output_scores=True)\n",
    "#generated = prompt + tokenizer.decode(outputs[0])[prompt_length:]\n",
    "\n",
    "p = torch.softmax(outputs.scores[0], dim=1)\n",
    "\n",
    "print(p.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d87a84ed",
   "metadata": {},
   "source": [
    "# WIP: Data augmentation with NER\n",
    "***\n",
    "Idea: Use Named entity recognition to find and replace persons etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04c818fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ailignment.datasets.moral_stories import get_moral_stories, make_action_classification_dataframe\n",
    "from ailignment import join_sentences, tokenize_and_split, get_accuracy_metric\n",
    "dataframe = get_moral_stories()\n",
    "columns = dataframe.columns[1:]\n",
    "print(\"Running NER on columns\", columns.to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4b146a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = join_sentences(dataframe ,columns, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d4beffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "ner_pipe = nlp.pipe(tqdm(texts), disable=['tagger', 'parser', 'attribute_ruler', 'lemmatizer'])\n",
    "docs = [x for x in ner_pipe]\n",
    "\n",
    "displacy.render(docs[0], style=\"ent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b61607f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def get_frequent_entity(doc, entity=\"PERSON\", n=1):\n",
    "    '''\n",
    "    Returns the highest number of occurences of an\n",
    "    entity in the NER doc.\n",
    "    '''\n",
    "    occurences = [(x.text, x.label_) for x in doc.ents if x.label_ == entity]\n",
    "    c = Counter(occurences)\n",
    "    ents = []\n",
    "    for item, count in c.most_common(n):\n",
    "        ents.append([x for x in doc.ents if (x.text, x.label_) == item])\n",
    "    \n",
    "    if n == 1 and len(ents) != 0:\n",
    "        ents = ents[0]\n",
    "    return ents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "965139d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "persons = [get_frequent_entity(x, \"PERSON\",n=1) for x in docs]\n",
    "# we are interested in the simplest case, where the NER found\n",
    "# exactly 6 matches\n",
    "matches = [x for x in persons if len(x) == 6]\n",
    "print(f\"Found {len(matches)} matches\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56082bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = matches[0]\n",
    "displacy.render(m[0].doc, \"ent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ded8d9e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_entity(ents, s):\n",
    "    '''\n",
    "    Replaces all occurences of entities in `ents` with `s`.\n",
    "    `ents` is a list of entities as returned by `doc.ents`\n",
    "    from an NER pipeline, they need to be from the same doc!\n",
    "    '''\n",
    "    offset = 0\n",
    "    text = ents[0].doc.text\n",
    "    new_text = \"\"\n",
    "    for ent in ents:\n",
    "        start = ent.start_char\n",
    "        end = ent.end_char\n",
    "        left = text[offset:start]\n",
    "        new_text += left + s\n",
    "        offset = end\n",
    "    new_text += text[offset:]\n",
    "    return new_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aae19b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_docs = [replace_entity(m, \"Niklas\").split(\"\\n\") for m in matches]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beee911a",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_docs = [m[0].doc.text.split(\"\\n\") for m in matches]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "863d1fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe_replaced = pd.DataFrame(n_docs)\n",
    "dataframe_replaced.columns = columns\n",
    "dataframe_replaced.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e1896b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "test_split = 0.2\n",
    "batch_size = 8\n",
    "\n",
    "action_dataframe = make_action_classification_dataframe(dataframe_replaced)\n",
    "\n",
    "input_columns = [\"action\"]\n",
    "action_dataframe[\"task_input\"] = join_sentences(action_dataframe, input_columns, \" \")\n",
    "dataset = datasets.Dataset.from_pandas(action_dataframe)\n",
    "dataset = dataset.train_test_split(test_size=test_split)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b5b78c1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "from transformers import (\n",
    "    AutoModelForSequenceClassification, DistilBertTokenizerFast,\n",
    "     Trainer, TrainingArguments, AutoModelWithLMHead, AutoTokenizer,\n",
    ")\n",
    "import torch\n",
    "\n",
    "model = \"distilbert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model)\n",
    "\n",
    "train_data, test_data = tokenize_and_split(dataset, tokenizer, \"task_input\")\n",
    "\n",
    "# for prototyping, optional\n",
    "small_train_data = train_data.shuffle(seed=42).select(range(1000))\n",
    "small_test_data = test_data.shuffle(seed=42).select(range(1000))\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"results/\",\n",
    "    num_train_epochs=5,              # total number of training epochs\n",
    "    per_device_train_batch_size=8,  # batch size per device during training\n",
    "    per_device_eval_batch_size=8,   # batch size for evaluation\n",
    "    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01,               # strength of weight decay\n",
    "    logging_dir='./logs',            # directory for storing logs\n",
    "    logging_steps=50,                # how often to log\n",
    "    save_steps=1000,\n",
    "    save_total_limit=0,\n",
    "    evaluation_strategy=\"epoch\",     # when to run evaluation\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e22e0723",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,                         # the instantiated ðŸ¤— Transformers model to be trained\n",
    "    args=training_args,                  # training arguments, defined above\n",
    "    train_dataset=small_train_data,   # training dataset\n",
    "    eval_dataset=small_test_data,     # evaluation dataset\n",
    "    compute_metrics=get_accuracy_metric,     # code to run accuracy metric\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ebbeaab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1711b948",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gender_guesser.detector import Detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12fd8975",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = Detector()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6774cae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "d.get_gender(\"Jamie\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ff68cec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ailignment.datasets.moral_stories import _lemmatize, get_moral_stories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2e4a88fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\kiehne\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from string import punctuation\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2fa31496",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm', disable=['parser', 'textcat'])\n",
    "STOP_WORDS = stopwords.words('english')\n",
    "stories = get_moral_stories()\n",
    "columns = [\"moral_action\", \"immoral_action\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9b4c2f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "series = stories[columns[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "29b71164",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_series(series, nlp, STOP_WORDS=None):\n",
    "    '''\n",
    "    Given a series of strings, returns a DataFrame([\"lemmas\", \"tokens\", \"maps\"])\n",
    "    of the lemmatized strings according to `_lemmatize` function.\n",
    "    '''\n",
    "    # get rid of whitespace\n",
    "    translation_table = str.maketrans(' ', ' ', punctuation)\n",
    "    series = series.map(lambda x: x.translate(translation_table))\n",
    "    series = series.map(lambda x: _lemmatize(x, nlp, STOP_WORDS))\n",
    "    data = pd.DataFrame(series.to_list(), columns=[\"lemmas\", \"tokens\", \"maps\"])\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "35699cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = series.map(lambda x: _lemmatize(x, nlp, STOP_WORDS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a3a3585e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame(y.to_list(), columns=[\"lemmas\", \"tokens\", \"maps\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3583c702",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lemmas</th>\n",
       "      <th>tokens</th>\n",
       "      <th>maps</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[kevin, decide, &lt;STPWRD&gt;, place, &lt;STPWRD&gt;, wag...</td>\n",
       "      <td>[Kevin, decides, to, place, a, wager, once, ev...</td>\n",
       "      <td>{0: [0], 1: [1], 2: [2], 3: [3], 4: [4], 5: [5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[curtis, explain, &lt;STPWRD&gt;, &lt;STPWRD&gt;, woman, &lt;...</td>\n",
       "      <td>[Curtis, explains, to, the, woman, that, he's,...</td>\n",
       "      <td>{0: [0], 1: [1], 2: [2], 3: [3], 4: [4], 5: [5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[anna, decide, &lt;STPWRD&gt;, attend, &lt;STPWRD&gt;, wed...</td>\n",
       "      <td>[Anna, decides, to, attend, the, wedding, even...</td>\n",
       "      <td>{0: [0], 1: [1], 2: [2], 3: [3], 4: [4], 5: [5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[greg, tell, james, &lt;STPWRD&gt;, &lt;STPWRD&gt;, &lt;STPWR...</td>\n",
       "      <td>[Greg, tells, James, that, he, can, play, tomo...</td>\n",
       "      <td>{0: [0], 1: [1], 2: [2], 3: [3], 4: [4], 5: [5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[ben, call, &lt;STPWRD&gt;, tow, truck, &lt;STPWRD&gt;, ge...</td>\n",
       "      <td>[Ben, calls, a, tow, truck, and, gets, the, ca...</td>\n",
       "      <td>{0: [0], 1: [1], 2: [2], 3: [3], 4: [4], 5: [5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11994</th>\n",
       "      <td>[payton, ask, &lt;STPWRD&gt;, friend, &lt;STPWRD&gt;, &lt;STP...</td>\n",
       "      <td>[Payton, asks, her, friend, where, she, bought...</td>\n",
       "      <td>{0: [0], 1: [1], 2: [2], 3: [3], 4: [4], 5: [5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11995</th>\n",
       "      <td>[helen, &lt;STPWRD&gt;, &lt;STPWRD&gt;, conversation, &lt;STP...</td>\n",
       "      <td>[Helen, has, a, conversation, with, her, husba...</td>\n",
       "      <td>{0: [0], 1: [1], 2: [2], 3: [3], 4: [4], 5: [5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11996</th>\n",
       "      <td>[cole, move, away, &lt;STPWRD&gt;, &lt;STPWRD&gt;, vehicle...</td>\n",
       "      <td>[Cole, moves, away, from, the, vehicle, that, ...</td>\n",
       "      <td>{0: [0], 1: [1], 2: [2], 3: [3], 4: [4], 5: [5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11997</th>\n",
       "      <td>[harry, ask, steve, &lt;STPWRD&gt;, &lt;STPWRD&gt;, &lt;STPWR...</td>\n",
       "      <td>[Harry, asks, Steve, what, he, should, be, doi...</td>\n",
       "      <td>{0: [0], 1: [1], 2: [2], 3: [3], 4: [4], 5: [5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11998</th>\n",
       "      <td>[andrew, confront, &lt;STPWRD&gt;, father, ,, &lt;STPWR...</td>\n",
       "      <td>[Andrew, confronts, his, father,, and, tells, ...</td>\n",
       "      <td>{0: [0], 1: [1], 2: [2], 3: [3], 4: [3], 5: [4...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11999 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  lemmas  \\\n",
       "0      [kevin, decide, <STPWRD>, place, <STPWRD>, wag...   \n",
       "1      [curtis, explain, <STPWRD>, <STPWRD>, woman, <...   \n",
       "2      [anna, decide, <STPWRD>, attend, <STPWRD>, wed...   \n",
       "3      [greg, tell, james, <STPWRD>, <STPWRD>, <STPWR...   \n",
       "4      [ben, call, <STPWRD>, tow, truck, <STPWRD>, ge...   \n",
       "...                                                  ...   \n",
       "11994  [payton, ask, <STPWRD>, friend, <STPWRD>, <STP...   \n",
       "11995  [helen, <STPWRD>, <STPWRD>, conversation, <STP...   \n",
       "11996  [cole, move, away, <STPWRD>, <STPWRD>, vehicle...   \n",
       "11997  [harry, ask, steve, <STPWRD>, <STPWRD>, <STPWR...   \n",
       "11998  [andrew, confront, <STPWRD>, father, ,, <STPWR...   \n",
       "\n",
       "                                                  tokens  \\\n",
       "0      [Kevin, decides, to, place, a, wager, once, ev...   \n",
       "1      [Curtis, explains, to, the, woman, that, he's,...   \n",
       "2      [Anna, decides, to, attend, the, wedding, even...   \n",
       "3      [Greg, tells, James, that, he, can, play, tomo...   \n",
       "4      [Ben, calls, a, tow, truck, and, gets, the, ca...   \n",
       "...                                                  ...   \n",
       "11994  [Payton, asks, her, friend, where, she, bought...   \n",
       "11995  [Helen, has, a, conversation, with, her, husba...   \n",
       "11996  [Cole, moves, away, from, the, vehicle, that, ...   \n",
       "11997  [Harry, asks, Steve, what, he, should, be, doi...   \n",
       "11998  [Andrew, confronts, his, father,, and, tells, ...   \n",
       "\n",
       "                                                    maps  \n",
       "0      {0: [0], 1: [1], 2: [2], 3: [3], 4: [4], 5: [5...  \n",
       "1      {0: [0], 1: [1], 2: [2], 3: [3], 4: [4], 5: [5...  \n",
       "2      {0: [0], 1: [1], 2: [2], 3: [3], 4: [4], 5: [5...  \n",
       "3      {0: [0], 1: [1], 2: [2], 3: [3], 4: [4], 5: [5...  \n",
       "4      {0: [0], 1: [1], 2: [2], 3: [3], 4: [4], 5: [5...  \n",
       "...                                                  ...  \n",
       "11994  {0: [0], 1: [1], 2: [2], 3: [3], 4: [4], 5: [5...  \n",
       "11995  {0: [0], 1: [1], 2: [2], 3: [3], 4: [4], 5: [5...  \n",
       "11996  {0: [0], 1: [1], 2: [2], 3: [3], 4: [4], 5: [5...  \n",
       "11997  {0: [0], 1: [1], 2: [2], 3: [3], 4: [4], 5: [5...  \n",
       "11998  {0: [0], 1: [1], 2: [2], 3: [3], 4: [3], 5: [4...  \n",
       "\n",
       "[11999 rows x 3 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "017f0c5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a3553355",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfbf5d72",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
