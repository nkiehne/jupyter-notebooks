{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a64ba63",
   "metadata": {},
   "source": [
    "# Loading the `Moral-Stories` dataset\n",
    "***\n",
    "The dataset and code can be found <a href=\"https://github.com/demelin/moral_stories\">here</a>.\\\n",
    "The authors provide 12k unique norms and, for some reason, additional 700k variations of the same norms, just with NaN fields every now and then. Zero additional information, but maybe I am overlooking something here?\n",
    "* Might be for different tasks? But then they only provide a single label which is always 1 for any NaN rows..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc517ac3",
   "metadata": {},
   "source": [
    "# Sample task: Action classification\n",
    "***\n",
    "For starters, let's reproduce a task from the paper:\n",
    "* Given an action, predict whether it is moral or immoral.\n",
    "* For simplicity, we do not use the splits introduced in the paper, but rather random splitting\n",
    "\n",
    "We start by loading the data as a `pandas.DataFrame`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0bfcfee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ailignment.datasets.moral_stories import get_moral_stories, make_action_classification_dataframe\n",
    "from ailignment.datasets import get_accuracy_metric, join_sentences, tokenize_and_split\n",
    "import pandas as pd\n",
    "import datasets\n",
    "import transformers\n",
    "import numpy as np\n",
    "from ailignment import sequence_classification\n",
    "\n",
    "#transformers.logging.set_verbosity_warning()\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "from tqdm import tqdm\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81fbcadc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataframe = get_moral_stories()\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "# run everything through spacy for part of speech tags\n",
    "dataframe[\"norm_parsed\"] = dataframe[\"norm\"].apply(nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cd3a074",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pos_until_verb(doc):\n",
    "    '''\n",
    "    Returns a tuple of Part-of-speech tags up until the first verb\n",
    "    Returns (\"EMPTY\") if no VERB is present\n",
    "    '''\n",
    "    l = [d.pos_ for d in doc]\n",
    "    if \"VERB\" not in l: return (\"EMPTY\",)\n",
    "    return tuple(l[:l.index(\"VERB\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aba2e048",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe[\"norm_pos\"] = dataframe[\"norm_parsed\"].apply(lambda x: [d.pos_ for d in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "874dec2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe[\"pos_verb\"] = dataframe[\"norm_parsed\"].apply(get_pos_until_verb)\n",
    "#pos_verb = dataframe.groupby(\"pos_verb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "917b38e1",
   "metadata": {},
   "source": [
    "# Untying the *is* from the *ought*\n",
    "***\n",
    "We are not to come up with new norms. Rather, we like to get rid of the usually ambiguous normative judgements. Hence, we aim to split the moral value assigned to a norm from the situation it applies to. Checking for norm violation then reduces to other well-known problems such as question answering or textual entailment.\n",
    "\n",
    "**TODO:** Surely there are smart names for the two categories of *situations that are judge-worthy* and their corresponding moral values.\n",
    "\n",
    "We look for the most common ways that norms are phrased:\n",
    "* \"You should\" or \"You shouldn't\"\n",
    "* \"It's\" or \"It is\" followed by good, bad, etc.\n",
    "\n",
    "Then, we want to translate them into \"Did you ....?\" and \"Well, that's ...\" pieces\n",
    "* E.g. \"You should back up so you can let people park.\" becomes\n",
    "    * \"Did you backup so you can let people park?\"\n",
    "    * \"Yes\" -> \"Good\"\n",
    "    * \"No\" -> \"Well, you should have!\"\n",
    "\n",
    "The norms only give value for either violation or adherence, but rarely both. We mirror the sentiment of the judgement by simple negation:\n",
    "* \"It is bad to hurt people\"\n",
    "    * \"Did you hurt people?\"\n",
    "    * \"Yes\" --> \"bad\"\n",
    "    * \"No\" --> \"not bad\"\n",
    "\n",
    "Note, that we refrain from translating \"not bad\" to \"good\", since norms often aren't morally symmetric, i.e. not every norm has equally strong normativity for violation and adherence. E.g. positively committing a crime is punished much more explicitly than is not committing it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a418f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "norms = dataframe[\"norm\"].apply(str.lower)\n",
    "\n",
    "f_should = lambda x: x.startswith(\"you should \") and not x.startswith(\"you should not\")\n",
    "shoulds = dataframe[norms.apply(f_should)]\n",
    "print(\"\\\"You should\\\":\", len(shoulds))\n",
    "\n",
    "f_shouldnt = lambda x: x.startswith(\"you shouldn\") or x.startswith(\"you should not\")\n",
    "shouldnts = dataframe[norms.apply(f_shouldnt)]\n",
    "print(\"\\\"You shouldnt\\\":\", len(shouldnts))\n",
    "\n",
    "f_its = lambda x: x.startswith(\"it's\") or x.startswith(\"it is\")\n",
    "its = dataframe[norms.apply(f_its)]\n",
    "print(\"\\\"It's\\\"\", len(its))\n",
    "\n",
    "total = len(its)+len(shoulds)+len(shouldnts)\n",
    "print(\"Covered\", total ,\"of\",len(dataframe), f\"({100*total/len(dataframe):.2f}%) norms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86432b16",
   "metadata": {},
   "source": [
    "### Untying `you should` sentences\n",
    "***\n",
    "First, find the most common grammatical structures:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75767217",
   "metadata": {},
   "outputs": [],
   "source": [
    "common_pos = shoulds[\"norm_pos\"].apply(lambda x: tuple(x[:3]))\n",
    "common_pos = shoulds.groupby(common_pos)\n",
    "most_common = common_pos[\"ID\"].count().sort_values(ascending=False)\n",
    "most_common"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e897feef",
   "metadata": {},
   "source": [
    "Only two seem to be of relevance:\n",
    "* \"You should VERB\"\n",
    "    * You should help the ones in need\n",
    "* \"You should ADV\"\n",
    "    * You should always, You should never\n",
    "\n",
    "While the first is straightforward to de-value, the latter at least requires more sophisticated reasoning capabilities concerning quantification and negation of cases.\n",
    "\n",
    "For now, we simply strip the initial \"You should\" from the former, and the \"You should ADV\" from the latter to obtain the judgements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea679817",
   "metadata": {},
   "outputs": [],
   "source": [
    "# do first group of (PRON, AUX, VERB)\n",
    "g0 = common_pos.get_group((\"PRON\",\"AUX\",\"VERB\")).copy()\n",
    "g0[\"norm_devalued\"] = g0[\"norm_parsed\"].apply(lambda x: x[2:].text)\n",
    "g0[\"value\"] = \"You should\"\n",
    "\n",
    "# second group of (PRON, AUX, ADV)\n",
    "g1 = common_pos.get_group((\"PRON\",\"AUX\",\"ADV\")).copy()\n",
    "g1[\"norm_devalued\"] = g1[\"norm_parsed\"].apply(lambda x: x[3:].text)\n",
    "g1[\"value\"] = g1[\"norm_parsed\"].apply(lambda x: \"You should \" + x[2].text)\n",
    "\n",
    "shoulds_devalued = pd.concat([g0,g1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c971880",
   "metadata": {},
   "source": [
    "### Untying `you should not` sentences\n",
    "***\n",
    "First, find the most common grammatical structures:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39f75c30",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "common_pos = shouldnts[\"norm_pos\"].apply(lambda x: tuple(x[:4]))\n",
    "common_pos = shouldnts.groupby(common_pos)\n",
    "most_common = common_pos[\"ID\"].count().sort_values(ascending=False)\n",
    "most_common"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6ee38bc",
   "metadata": {},
   "source": [
    "Only one seems to be of relevance:\n",
    "* \"You should not VERB\"\n",
    "    * You should not spit in someone's face\n",
    "\n",
    "Similarly to the \"You should\" cases, we strip \"You should not\" as the value-judgement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a2c1a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "shouldnts_devalued = common_pos.get_group((\"PRON\",\"AUX\",\"PART\", \"VERB\")).copy()\n",
    "shouldnts_devalued[\"norm_devalued\"] = shouldnts_devalued[\"norm_parsed\"].apply(lambda x: x[3:].text)\n",
    "shouldnts_devalued[\"value\"] = \"You should not\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6c9dd8b",
   "metadata": {},
   "source": [
    "### Untying `it is` sentences\n",
    "***\n",
    "First, find the most common grammatical structures up until the first verb:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb469c19",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#common_pos = its[\"norm_pos\"].apply(lambda x: tuple(x[:5]))\n",
    "common_pos = its.groupby(\"pos_verb\")\n",
    "most_common = common_pos[\"ID\"].count().sort_values(ascending=False)\n",
    "most_common[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b3e399",
   "metadata": {},
   "source": [
    "The situation is a little more complex here, it seems:\n",
    "1. \"It is ADJ to\", It's wrong to become addicted to gambling.\n",
    "    * Value: ADJ, strip until first verb\n",
    "2. \"It is not ADJ to\", It's not okay to invade someone else's privacy.\n",
    "    * Like 1.\n",
    "3. (PRON, AUX, ADJ, PART, PART). Two subgroups, which are both currently ignored\n",
    "    1. \"It is ADJ not to\"\n",
    "    2. \"It is ADJ to not\"\n",
    "\n",
    "Similarly to the \"You should\" cases, we strip \"You should not\" as the value-judgement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd2a8caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "groups = []\n",
    "# (PRON, AUX, ADJ, PART)\n",
    "g = common_pos.get_group((\"PRON\",\"AUX\",\"ADJ\", \"PART\")).copy()\n",
    "g[\"norm_devalued\"] = g[\"norm_parsed\"].apply(lambda x: x[4:].text)\n",
    "g[\"value\"] = g[\"norm_parsed\"].apply(lambda x: x[2].text)\n",
    "groups.append(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb583ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (PRON, AUX, PART, ADJ, PART)\n",
    "g = common_pos.get_group((\"PRON\",\"AUX\",\"PART\",\"ADJ\", \"PART\")).copy()\n",
    "g[\"norm_devalued\"] = g[\"norm_parsed\"].apply(lambda x: x[5:].text)\n",
    "g[\"value\"] = g[\"norm_parsed\"].apply(lambda x: x[2:4].text)\n",
    "groups.append(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e32fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (PRON, AUX, ADJ, PART, PART)\n",
    "g = common_pos.get_group((\"PRON\",\"AUX\",\"ADJ\",\"PART\", \"PART\")).copy()\n",
    "g[\"norm_devalued\"] = g[\"norm_parsed\"].apply(lambda x: x[5:].text)\n",
    "g[\"value\"] = g[\"norm_parsed\"].apply(lambda x: x[2:4].text)\n",
    "# currently ignored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12d5d616",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (PRON, AUX)\n",
    "g = common_pos.get_group((\"PRON\",\"AUX\")).copy()\n",
    "g[\"norm_devalued\"] = g[\"norm_parsed\"].apply(lambda x: x[5:].text)\n",
    "g[\"value\"] = g[\"norm_parsed\"].apply(lambda x: x[2:4].text)\n",
    "# currently ignored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc7faa24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# finally, stitch everything together\n",
    "its_devalued = pd.concat(groups)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b797163",
   "metadata": {},
   "source": [
    "# Imperatives and values\n",
    "***\n",
    "By splitting up value-judgements from the norms we end up with imperatives describing what the action looks like and the value one should expect to receive from it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4aebee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe_devalued = pd.concat([shoulds_devalued, shouldnts_devalued, its_devalued])\n",
    "total_split = len(dataframe_devalued)\n",
    "print(\"After devaluation, we cover\", total_split ,\"of\",len(dataframe), f\"({100*total_split/len(dataframe):.2f}%) norms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94398722",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataframe_devalued[[\"moral_action\",\"norm_devalued\",\"value\"]].sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7ff1e9f",
   "metadata": {},
   "source": [
    "# Vertical protoype\n",
    "***\n",
    "Before bothering with the norms that were left out during the above steps, I'd like to show a proof of concept. Therefore, the next steps are:\n",
    "1. Derive a set of subjectified actions from the normative imperatives. I will do this with a table of conjugated verbs and handcrafted rules for the pronouns you, your and yourself. What about the names??\n",
    "2. Run the textual entailment experiment between pairs of subjectified action and either moral or immoral action"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c179eced",
   "metadata": {},
   "source": [
    "## Subjectified actions prototype\n",
    "***\n",
    "1. Get a list of conjugated verbs\n",
    "2. Get all norms whose imperative verb is contained in the list\n",
    "3. Find the actor's name from the actions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c94354",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load a list of conjugated english verbs from https://github.com/monolithpl/verb.forms.dictionary\n",
    "verbs = pd.read_csv(\"https://raw.githubusercontent.com/monolithpl/verb.forms.dictionary/master/csv/verbs-dictionaries.csv\",\n",
    "                   sep=\"\\t\", \n",
    "                    names=[\"present simple 1st\",\"present simple 3rd\",\"past simple\",\"past participle\",\"present participle\"])\n",
    "base_forms = set(verbs[\"present simple 1st\"].to_list())\n",
    "verb_map = {a:b for i,(a,b) in verbs[verbs.columns[:2]].iterrows()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08f59230",
   "metadata": {},
   "outputs": [],
   "source": [
    "# re run pos tagging on the imperatives\n",
    "dataframe_devalued[\"norm_devalued_pos\"] = dataframe_devalued[\"norm_devalued\"].apply(nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b935f402",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter out all norms for which we do not have an entry in our verb list\n",
    "norm_verbs = dataframe_devalued[\"norm_devalued_pos\"].apply(lambda x: x[0].text)\n",
    "dataframe_devalued = dataframe_devalued[norm_verbs.apply(lambda x: x in base_forms)]\n",
    "print(\"After filtering out unknown verbs, we cover\", len(dataframe_devalued) ,\"of\",len(dataframe), \n",
    "      f\"({100*len(dataframe_devalued)/len(dataframe):.2f}%) norms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1ab2a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply pos tagging to moral actions to find sentences starting with a name\n",
    "dataframe_devalued[\"moral_action_parsed\"] = dataframe_devalued[\"moral_action\"].apply(nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4f9604f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the most common pos tags at the start of the actions\n",
    "pos_groups = dataframe_devalued.groupby(dataframe_devalued[\"moral_action_parsed\"].apply(lambda x: x[0].pos_))\n",
    "counts = pos_groups[\"ID\"].count().sort_values(ascending=False)\n",
    "print(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0551b691",
   "metadata": {},
   "outputs": [],
   "source": [
    "# after visual examination, the first three categories seem to only contain names\n",
    "# pos_groups[\"moral_action\"].get_group(counts.index[2]).to_list()\n",
    "dataframe_devalued = pd.concat([pos_groups.get_group(counts.index[i]) for i in [0,1,2]])\n",
    "print(\"After focusing on known names, we cover\", len(dataframe_devalued) ,\"of\",len(dataframe), \n",
    "      f\"({100*len(dataframe_devalued)/len(dataframe):.2f}%) norms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "813209b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# next up, gather actor names\n",
    "dataframe_devalued[\"actor_name\"] = dataframe_devalued[\"moral_action_parsed\"].apply(lambda x: x[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f91fe03e",
   "metadata": {},
   "source": [
    "### Switching persons\n",
    "***\n",
    "We need to also translate the pronouns in the imperatives from second (you, your) to third person (his/hers, him/her). Unfortunately, this requires the gender of the actor.\n",
    "\n",
    "Here, we use package `gender-guesser`, as found here https://github.com/lead-ratings/gender-guesser\n",
    "\n",
    "It assigns either andy, female, male, mostly female/male or unknown gender.\n",
    "\n",
    "Once again, the ambiguity of the english language hits us:\n",
    "* \"you\" might either stand for he/she or him/her\n",
    "* Since only around 400 rows are affected, we ignore them for the protoype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86d1c3fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gender_guesser.detector as gg\n",
    "det = gg.Detector()\n",
    "dataframe_devalued[\"actor_gender\"] = dataframe_devalued[\"actor_name\"].apply(lambda x: det.get_gender(x,\"usa\"))\n",
    "print(dataframe_devalued[\"actor_gender\"].groupby(dataframe_devalued[\"actor_gender\"]).count().sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "051568c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter on genders\n",
    "dataframe_devalued[\"actor_gender\"].replace(\"mostly_male\",\"male\", inplace=True)\n",
    "dataframe_devalued[\"actor_gender\"].replace(\"mostly_female\",\"female\", inplace=True)\n",
    "\n",
    "dataframe_devalued = dataframe_devalued[dataframe_devalued[\"actor_gender\"].apply(lambda x: \"male\" in x)]\n",
    "print(\"After focusing on known genders, we cover\", len(dataframe_devalued) ,\"of\",len(dataframe), \n",
    "      f\"({100*len(dataframe_devalued)/len(dataframe):.2f}%) norms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efbc59dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get rid of all sentences with a \"you\", since we can't properly conjugate(?) it\n",
    "dataframe_devalued = dataframe_devalued[dataframe_devalued[\"norm_devalued\"].apply(lambda x: \" you \" not in x and not x.endswith(\" you\"))]\n",
    "print(\"After filtering 'you's from norms, we cover\", len(dataframe_devalued) ,\"of\",len(dataframe), \n",
    "      f\"({100*len(dataframe_devalued)/len(dataframe):.2f}%) norms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9a71f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we check whether a norm story contains any 2nd person pronouns\n",
    "male_pron_map = {\"you\":\"he\",\"your\":\"his\",\"yours\":\"his\",\"yourself\":\"himself\"}\n",
    "female_pron_map = {\"you\":\"she\",\"your\":\"her\",\"yours\":\"hers\",\"yourself\":\"herself\"}\n",
    "\n",
    "def translate_row(row):\n",
    "    # actor name + 3rd person verb + rest of imperative\n",
    "    # replace pronouns with 3rd person version\n",
    "    verb = verb_map[row[\"norm_devalued_pos\"][0].text]\n",
    "    \n",
    "    doc = row[\"norm_devalued_pos\"]\n",
    "    pron_map = male_pron_map if row[\"actor_gender\"] == \"male\" else female_pron_map\n",
    "    imperative = doc[:len(doc) if not doc[-1].is_punct else len(doc)-1]\n",
    "    imperative = \" \".join([pron_map.get(x.text, x.text) for x in imperative[1:]])\n",
    "\n",
    "    story = \" \".join([row[\"actor_name\"], verb, imperative])\n",
    "    return story\n",
    "\n",
    "\n",
    "dataframe_devalued[\"norm_story\"] = dataframe_devalued.apply(translate_row, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2d7661f",
   "metadata": {},
   "source": [
    "# Testing the prototype\n",
    "***\n",
    "So far, we obtained several situations of people actually performing the norm, which we call `norm-story`. Now, we'd like to test, whether natural language inference (NLI), or more specifically, textual entailment models are able to tell apart the moral action from the immoral ones:\n",
    "* Norm: *It's manipulative to try to force a partner into marriage.*\n",
    "* Norm-story: *Jake tries to force a partner into marriage*\n",
    "* Moral action: *Jake proposes to Harry at the bar they met at.*\n",
    "* Immoral action: *Jake tells Harry that he will kill himself if he doesn't marry him.*\n",
    "\n",
    "General idea: If an action $A$ entails the norm-story $A_N$ we assume that the actor also performed $A_N$ and is therefore exposed to the value-judgement of the norm. Continuing the above example:\n",
    "* If we find that $A=$*Jake proposes to Harry at the bar they met at.* is a sufficient condition for the statement $A_N=$*Jake proposes to Harry at the bar they met at.*, then we assign the value *manipulative* to $A$.\n",
    "\n",
    "In order to decide whether $A$ is moral or immoral, the worded value (e.g. manipulative, good, bad) needs to be interpretet as moral or immoral. Here we make the case, that sentiment analysis expresses whether a word was meant to be positive or negative. Consequently, we use the estimated sentiments as numerical normative values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "772280b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimate sentiments using huggingface\n",
    "from transformers import pipeline\n",
    "classifier = pipeline('sentiment-analysis')\n",
    "value_map = {v:classifier(v)[0][\"label\"] for v in dataframe_devalued[\"value\"].unique()}\n",
    "dataframe_devalued[\"norm_sentiment\"] = dataframe_devalued[\"value\"].apply(value_map.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32ac24ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save a stripped version of the data for later use\n",
    "dataframe_devalued.to_pickle(\"../data/moral_stories_proto.dat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "499392ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe_devalued = pd.read_pickle(\"../data/moral_stories_proto.dat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c58d11d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ynie/roberta-large-snli_mnli_fever_anli_R1_R2_R3-nli were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# load the NLI model and its tokenizer\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "name = \"ynie/roberta-large-snli_mnli_fever_anli_R1_R2_R3-nli\" # 85%\n",
    "#name = 'cross-encoder/nli-distilroberta-base' # 80%\n",
    "#name = \"boychaboy/SNLI_bert-base-uncased\" # 75%\n",
    "tokenizer = AutoTokenizer.from_pretrained(name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b15e098e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "626409343f9443949484f0d8a8930a0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# create a huggingface dataset and setup metrics\n",
    "from datasets import Dataset\n",
    "\n",
    "def tok(samples):\n",
    "    return tokenizer(samples[\"moral_action\"], samples[\"norm_story\"], padding=\"max_length\", \n",
    "                     truncation=True, return_token_type_ids=True)\n",
    "\n",
    "data = dataframe_devalued[[\"moral_action\", \"norm_story\"]].copy()\n",
    "data[\"label\"] = dataframe_devalued[\"norm_sentiment\"].apply(lambda x: int(x==\"POSITIVE\"))\n",
    "dataset = Dataset.from_pandas(data)\n",
    "dataset = dataset.map(tok, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "74d03c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_set = dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "646835ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the test set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: __index_level_0__, moral_action, norm_story.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 8445\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='528' max='528' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [528/528 10:48]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8525754884547069\n"
     ]
    }
   ],
   "source": [
    "# run evaluation\n",
    "from transformers import Trainer, TrainingArguments\n",
    "import torch\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"results/\",\n",
    "    num_train_epochs=0,              # total number of training epochs\n",
    "    per_device_train_batch_size=1,  # batch size per device during training\n",
    "    per_device_eval_batch_size=16,   # batch size for evaluation\n",
    "    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01,               # strength of weight decay\n",
    "    logging_dir='./logs',            # directory for storing logs\n",
    "    logging_steps=50,                # how often to log\n",
    "    evaluation_strategy=\"epoch\",     # when to run evaluation\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    ")\n",
    "results = trainer.predict(eval_set)\n",
    "scores = torch.softmax(torch.from_numpy(results.predictions),1).numpy()\n",
    "\n",
    "is_entailed = (scores[:,0] > scores[:,2]).astype(\"int32\")\n",
    "labels = np.array(eval_set[\"label\"])\n",
    "acc = (is_entailed == labels).sum()/len(labels)\n",
    "print(\"Accuracy:\", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8b6dd00f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.4389579632918887\n"
     ]
    }
   ],
   "source": [
    "is_entailed = (scores[:,2] > scores[:,1]).astype(\"int32\")\n",
    "labels = np.array(eval_set[\"label\"])\n",
    "acc = (is_entailed == labels).sum()/len(labels)\n",
    "print(\"Accuracy:\", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7f6592a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "loading configuration file https://huggingface.co/distilbert-base-uncased/resolve/main/config.json from cache at C:\\Users\\nikla/.cache\\huggingface\\transformers\\23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.91b885ab15d631bf9cee9dc9d25ece0afd932f2f5130eba28f2055b2220c0333\n",
      "Model config DistilBertConfig {\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.9.2\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/distilbert-base-uncased/resolve/main/vocab.txt from cache at C:\\Users\\nikla/.cache\\huggingface\\transformers\\0e1bbfda7f63a99bb52e3915dcf10c3c92122b827d92eb2d34ce94ee79ba486c.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
      "loading file https://huggingface.co/distilbert-base-uncased/resolve/main/tokenizer.json from cache at C:\\Users\\nikla/.cache\\huggingface\\transformers\\75abb59d7a06f4f640158a9bfcde005264e59e8d566781ab1415b139d2e4c603.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
      "loading file https://huggingface.co/distilbert-base-uncased/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/distilbert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/distilbert-base-uncased/resolve/main/tokenizer_config.json from cache at C:\\Users\\nikla/.cache\\huggingface\\transformers\\8c8624b8ac8aa99c60c912161f8332de003484428c47906d7ff7eb7f73eecdbb.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n",
      "loading configuration file https://huggingface.co/distilbert-base-uncased/resolve/main/config.json from cache at C:\\Users\\nikla/.cache\\huggingface\\transformers\\23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.91b885ab15d631bf9cee9dc9d25ece0afd932f2f5130eba28f2055b2220c0333\n",
      "Model config DistilBertConfig {\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.9.2\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading configuration file https://huggingface.co/distilbert-base-uncased/resolve/main/config.json from cache at C:\\Users\\nikla/.cache\\huggingface\\transformers\\23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.91b885ab15d631bf9cee9dc9d25ece0afd932f2f5130eba28f2055b2220c0333\n",
      "Model config DistilBertConfig {\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.9.2\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/distilbert-base-uncased/resolve/main/pytorch_model.bin from cache at C:\\Users\\nikla/.cache\\huggingface\\transformers\\9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.bias', 'vocab_layer_norm.weight', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_projector.weight', 'vocab_transform.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.bias', 'classifier.bias', 'pre_classifier.weight', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14571ba7e4c74fd0bdcfb2efda4e621a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/14 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9f7d0485e7442609cd75921c0b9fada",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: intention, norm, action, situation, ID, task_input, consequence.\n",
      "***** Running training *****\n",
      "  Num examples = 13512\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 12\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 12\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 3378\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3378' max='3378' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3378/3378 31:00, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.522400</td>\n",
       "      <td>0.489160</td>\n",
       "      <td>0.780935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.345300</td>\n",
       "      <td>0.471897</td>\n",
       "      <td>0.790705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.193300</td>\n",
       "      <td>0.762193</td>\n",
       "      <td>0.799882</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: intention, norm, action, situation, ID, task_input, consequence.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3378\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: intention, norm, action, situation, ID, task_input, consequence.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3378\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: intention, norm, action, situation, ID, task_input, consequence.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3378\n",
      "  Batch size = 12\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7809354647720544, 0.790704558910598, 0.7998815867377146]\n"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "import torch\n",
    "\n",
    "dataframe = dataframe_devalued[dataframe_devalued.columns[:8]]\n",
    "test_split = 0.2\n",
    "batch_size = 12\n",
    "model = \"distilbert-base-uncased\"\n",
    "#model = \"albert-base-v2\"\n",
    "action_dataframe = make_action_classification_dataframe(dataframe)\n",
    "input_columns = [\"norm\", \"action\"]\n",
    "action_dataframe[\"task_input\"] = join_sentences(action_dataframe, input_columns)\n",
    "dataset = datasets.Dataset.from_pandas(action_dataframe)\n",
    "dataset = dataset.train_test_split(test_size=test_split)\n",
    "\n",
    "def data_all(tokenizer):\n",
    "    return tokenize_and_split(dataset, tokenizer, \"task_input\")\n",
    "def data_small(tokenizer):\n",
    "    train, test = data_all(tokenizer)\n",
    "    train = train.shuffle(seed=42).select(range(1000))\n",
    "    test = test.shuffle(seed=42).select(range(1000))\n",
    "    return train, test\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"results/\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    warmup_steps=500,\n",
    "    #weight_decay=wd,\n",
    "    #learning_rate=lr,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=500,\n",
    "    save_steps=1000000,\n",
    "    save_total_limit=0,\n",
    "    evaluation_strategy=\"epoch\",\n",
    ")\n",
    "\n",
    "r = sequence_classification(data_all, model, training_args, get_accuracy_metric())\n",
    "acc = [x[\"eval_accuracy\"] for x in r if \"eval_accuracy\" in x]\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "157df0bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "[[x[\"eval_accuracy\"] for x in r if \"eval_accuracy\" in x] for r in results]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ce840a5",
   "metadata": {},
   "source": [
    "# WIP: Get score output from LM\n",
    "***\n",
    "Question: Is there a better way to sample from generated LM outputs?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e617675",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    AutoModelForSequenceClassification, DistilBertTokenizerFast,\n",
    "     Trainer, TrainingArguments, AutoModelWithLMHead, AutoTokenizer,\n",
    ")\n",
    "import torch\n",
    "\n",
    "model = \"distilbert-base-uncased\"\n",
    "model = \"gpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "model = AutoModelWithLMHead.from_pretrained(model)\n",
    "\n",
    "prompt = \"Today the weather is really nice and I am planning on \"\n",
    "inputs = tokenizer.encode(prompt, add_special_tokens=False, return_tensors=\"pt\")\n",
    "\n",
    "prompt_length = len(tokenizer.decode(inputs[0], skip_special_tokens=True, clean_up_tokenization_spaces=True))\n",
    "outputs = model.generate(inputs, max_length=250, do_sample=False, top_p=0.95, top_k=60,\n",
    "                        return_dict_in_generate=True, output_attentions=False,\n",
    "                        output_hidden_states=True, output_scores=True)\n",
    "#generated = prompt + tokenizer.decode(outputs[0])[prompt_length:]\n",
    "\n",
    "p = torch.softmax(outputs.scores[0], dim=1)\n",
    "\n",
    "print(p.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d87a84ed",
   "metadata": {},
   "source": [
    "# WIP: Data augmentation with NER\n",
    "***\n",
    "Idea: Use Named entity recognition to find and replace persons etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04c818fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ailignment.datasets.moral_stories import get_moral_stories, make_action_classification_dataframe\n",
    "from ailignment import join_sentences, tokenize_and_split, get_accuracy_metric\n",
    "dataframe = get_moral_stories()\n",
    "columns = dataframe.columns[1:]\n",
    "print(\"Running NER on columns\", columns.to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4b146a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = join_sentences(dataframe ,columns, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d4beffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "ner_pipe = nlp.pipe(tqdm(texts), disable=['tagger', 'parser', 'attribute_ruler', 'lemmatizer'])\n",
    "docs = [x for x in ner_pipe]\n",
    "\n",
    "displacy.render(docs[0], style=\"ent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b61607f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def get_frequent_entity(doc, entity=\"PERSON\", n=1):\n",
    "    '''\n",
    "    Returns the highest number of occurences of an\n",
    "    entity in the NER doc.\n",
    "    '''\n",
    "    occurences = [(x.text, x.label_) for x in doc.ents if x.label_ == entity]\n",
    "    c = Counter(occurences)\n",
    "    ents = []\n",
    "    for item, count in c.most_common(n):\n",
    "        ents.append([x for x in doc.ents if (x.text, x.label_) == item])\n",
    "    \n",
    "    if n == 1 and len(ents) != 0:\n",
    "        ents = ents[0]\n",
    "    return ents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "965139d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "persons = [get_frequent_entity(x, \"PERSON\",n=1) for x in docs]\n",
    "# we are interested in the simplest case, where the NER found\n",
    "# exactly 6 matches\n",
    "matches = [x for x in persons if len(x) == 6]\n",
    "print(f\"Found {len(matches)} matches\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56082bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = matches[0]\n",
    "displacy.render(m[0].doc, \"ent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ded8d9e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_entity(ents, s):\n",
    "    '''\n",
    "    Replaces all occurences of entities in `ents` with `s`.\n",
    "    `ents` is a list of entities as returned by `doc.ents`\n",
    "    from an NER pipeline, they need to be from the same doc!\n",
    "    '''\n",
    "    offset = 0\n",
    "    text = ents[0].doc.text\n",
    "    new_text = \"\"\n",
    "    for ent in ents:\n",
    "        start = ent.start_char\n",
    "        end = ent.end_char\n",
    "        left = text[offset:start]\n",
    "        new_text += left + s\n",
    "        offset = end\n",
    "    new_text += text[offset:]\n",
    "    return new_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aae19b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_docs = [replace_entity(m, \"Niklas\").split(\"\\n\") for m in matches]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beee911a",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_docs = [m[0].doc.text.split(\"\\n\") for m in matches]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "863d1fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe_replaced = pd.DataFrame(n_docs)\n",
    "dataframe_replaced.columns = columns\n",
    "dataframe_replaced.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e1896b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "test_split = 0.2\n",
    "batch_size = 8\n",
    "\n",
    "action_dataframe = make_action_classification_dataframe(dataframe_replaced)\n",
    "\n",
    "input_columns = [\"action\"]\n",
    "action_dataframe[\"task_input\"] = join_sentences(action_dataframe, input_columns, \" \")\n",
    "dataset = datasets.Dataset.from_pandas(action_dataframe)\n",
    "dataset = dataset.train_test_split(test_size=test_split)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b5b78c1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "from transformers import (\n",
    "    AutoModelForSequenceClassification, DistilBertTokenizerFast,\n",
    "     Trainer, TrainingArguments, AutoModelWithLMHead, AutoTokenizer,\n",
    ")\n",
    "import torch\n",
    "\n",
    "model = \"distilbert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model)\n",
    "\n",
    "train_data, test_data = tokenize_and_split(dataset, tokenizer, \"task_input\")\n",
    "\n",
    "# for prototyping, optional\n",
    "small_train_data = train_data.shuffle(seed=42).select(range(1000))\n",
    "small_test_data = test_data.shuffle(seed=42).select(range(1000))\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"results/\",\n",
    "    num_train_epochs=5,              # total number of training epochs\n",
    "    per_device_train_batch_size=8,  # batch size per device during training\n",
    "    per_device_eval_batch_size=8,   # batch size for evaluation\n",
    "    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01,               # strength of weight decay\n",
    "    logging_dir='./logs',            # directory for storing logs\n",
    "    logging_steps=50,                # how often to log\n",
    "    save_steps=1000,\n",
    "    save_total_limit=0,\n",
    "    evaluation_strategy=\"epoch\",     # when to run evaluation\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e22e0723",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,                         # the instantiated  Transformers model to be trained\n",
    "    args=training_args,                  # training arguments, defined above\n",
    "    train_dataset=small_train_data,   # training dataset\n",
    "    eval_dataset=small_test_data,     # evaluation dataset\n",
    "    compute_metrics=get_accuracy_metric,     # code to run accuracy metric\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ebbeaab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1711b948",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gender_guesser.detector import Detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12fd8975",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = Detector()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6774cae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "d.get_gender(\"Jamie\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff68cec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ailignment.datasets.moral_stories import _lemmatize, get_moral_stories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e4a88fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from string import punctuation\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fa31496",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm', disable=['parser', 'textcat'])\n",
    "STOP_WORDS = stopwords.words('english')\n",
    "stories = get_moral_stories()\n",
    "columns = [\"moral_action\", \"immoral_action\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b4c2f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "series = stories[columns[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29b71164",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_series(series, nlp, STOP_WORDS=None):\n",
    "    '''\n",
    "    Given a series of strings, returns a DataFrame([\"lemmas\", \"tokens\", \"maps\"])\n",
    "    of the lemmatized strings according to `_lemmatize` function.\n",
    "    '''\n",
    "    # get rid of whitespace\n",
    "    translation_table = str.maketrans(' ', ' ', punctuation)\n",
    "    series = series.map(lambda x: x.translate(translation_table))\n",
    "    series = series.map(lambda x: _lemmatize(x, nlp, STOP_WORDS))\n",
    "    data = pd.DataFrame(series.to_list(), columns=[\"lemmas\", \"tokens\", \"maps\"])\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35699cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = series.map(lambda x: _lemmatize(x, nlp, STOP_WORDS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3a3585e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame(y.to_list(), columns=[\"lemmas\", \"tokens\", \"maps\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3583c702",
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "017f0c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3553355",
   "metadata": {},
   "outputs": [],
   "source": [
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfbf5d72",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
