{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decreased-geology",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "%pip install transformers sklearn datasets ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "certain-overall",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "for gpu in gpus:\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "tf.autograph.set_verbosity(2)\n",
    "'''\n",
    "\n",
    "from tqdm import tqdm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "micro-waterproof",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Challenge 3: Sentiment analysis\n",
    "***\n",
    "We are interested in predicting the sentiment of written text.\n",
    "* For the challenge, we adopt the IMDb dataset of movie reviews:\n",
    "    * \"[...] *this film is very lovable in a way many comedies are not* '[...]\"\n",
    "\n",
    "The task is simple:\n",
    "* Predict whether a review has a positive or a negative sentiment\n",
    "    * Input: Paragraphs of text (string) and binary label (1: positive, 0: negative)\n",
    "    * Metric: Accuracy\n",
    "* Examples to get you started:\n",
    "    * Finetune transformer models, e.g. BERT\n",
    "    * Word2Vec + Deep Neural Network of your choice"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reflected-alabama",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Hints\n",
    "***\n",
    "When training transformers:\n",
    "* The models are *huge*, so training will run very slowly\n",
    "    * Running several batches of the full training data will be a costly operation\n",
    "    * Google Colab needs you to do a captcha every 2h...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "celtic-invasion",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Loading the data\n",
    "***\n",
    "Using Huggingface `datasets` library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "inappropriate-adapter",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "raw_datasets = load_dataset(\"imdb\")\n",
    "print(raw_datasets[\"train\"][\"text\"][2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "italic-inspector",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "# Dataset preprocessing\n",
    "***\n",
    "The function below translates the sentences to token IDs and splits into train and validation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "functioning-terminal",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def tokenize_and_split(datasets, tokenizer):\n",
    "\n",
    "    def tokenize_function(examples):\n",
    "        return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "    tokenized_datasets = datasets.map(tokenize_function, batched=True)\n",
    "    train_dataset = tokenized_datasets[\"train\"]\n",
    "    eval_dataset = tokenized_datasets[\"test\"]\n",
    "    return train_dataset, eval_dataset\n",
    "\n",
    "def as_tf_dataset(dataset, tokenizer, batch_size=8):\n",
    "    tf_data = dataset.remove_columns([\"text\"]).with_format(\"tensorflow\")\n",
    "    train_features = {x: tf_data[x].to_tensor() for x in tokenizer.model_input_names}\n",
    "    tf_dataset = tf.data.Dataset.from_tensor_slices((train_features, tf_data[\"label\"]))\n",
    "    tf_dataset = tf_dataset.shuffle(len(tf_dataset)).batch(batch_size)\n",
    "    return tf_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "covered-valuation",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Finetuning transformers\n",
    "***\n",
    "Huggingface's `transformers` library provides excellent functionality and many pretrained models\n",
    "* Let's load a smaller variant of BERT, called `DistilBert` and its tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pursuant-encoding",
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import DistilBertTokenizerFast\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained(\"distilbert-base-uncased\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "great-adoption",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Run tokenization\n",
    "***\n",
    "Apply tokenizer and sample a small subset for showcasing the workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alien-singapore",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "train_data, test_data = tokenize_and_split(raw_datasets, tokenizer)\n",
    "small_train_dataset = train_data.shuffle(seed=42).select(range(1000))\n",
    "small_eval_dataset = test_data.shuffle(seed=42).select(range(1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "copyrighted-fiber",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Variant 1: Train using `keras`\n",
    "***\n",
    "We need to convert to a dataset format that `keras` understands:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beginning-falls",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "tf_train_small = as_tf_dataset(small_train_dataset, tokenizer, batch_size=8)\n",
    "tf_eval_small = as_tf_dataset(small_eval_dataset, tokenizer, batch_size=8)\n",
    "print(tf_train_small)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "monthly-cleanup",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Loading the model\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "future-biodiversity",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import TFDistilBertForSequenceClassification\n",
    "model = TFDistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=2)\n",
    "model.summary(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "systematic-season",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Time to train (`keras`)\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "passing-advisory",
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=5e-5),\n",
    "    loss= keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=tf.metrics.SparseCategoricalAccuracy(),\n",
    ")\n",
    "\n",
    "model.fit(tf_train_small, validation_data=tf_eval_small, epochs=3, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "olive-zimbabwe",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Evaluation code for `keras`\n",
    "***\n",
    "We run the final evaluation on the full test dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "serious-settle",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "tf_eval_full = as_tf_dataset(test_data, tokenizer, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nonprofit-attack",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "loss, acc = model.evaluate(tf_eval_full)\n",
    "print(f\"Reached {acc:.3f} accuracy and a loss of {loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "loaded-brunswick",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Variant 2: Train using PyTorch\n",
    "***\n",
    "The `transformers` framework comes with its own `Trainer` class which we can use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "editorial-headquarters",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import DistilBertForSequenceClassification, Trainer, TrainingArguments\n",
    "\n",
    "model = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prospective-faculty",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Specifying training arguments\n",
    "***\n",
    "The usual hyperparameters can be set using `TrainingArguments`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mental-bobby",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"results/\",\n",
    "    num_train_epochs=3,              # total number of training epochs\n",
    "    per_device_train_batch_size=8,  # batch size per device during training\n",
    "    per_device_eval_batch_size=8,   # batch size for evaluation\n",
    "    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01,               # strength of weight decay\n",
    "    logging_dir='./logs',            # directory for storing logs\n",
    "    logging_steps=50,                # how often to log\n",
    "    evaluation_strategy=\"epoch\",     # when to run evaluation\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "promising-serve",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "# Adding accuracy metric\n",
    "***\n",
    "Evaluation workflow is a little different to what we are used from `keras`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reasonable-rough",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from datasets import load_metric\n",
    "\n",
    "metric = load_metric(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "industrial-renaissance",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Time to train (PyTorch)\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aerial-phase",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,                         # the instantiated ðŸ¤— Transformers model to be trained\n",
    "    args=training_args,                  # training arguments, defined above\n",
    "    train_dataset=small_train_dataset,   # training dataset\n",
    "    eval_dataset=small_eval_dataset,     # evaluation dataset\n",
    "    compute_metrics=compute_metrics,     # code to run accuracy metric\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "worse-socket",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Evaluation code for PyTorch\n",
    "***\n",
    "We simply define a new `Trainer` that runs on the complete `eval_data`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exposed-andrews",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=small_train_dataset,\n",
    "    eval_dataset=test_data,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "results = trainer.evaluate()\n",
    "loss, acc = results[\"eval_loss\"], results[\"eval_accuracy\"]\n",
    "print(f\"Reached {acc:.3f} accuracy and a loss of {loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spoken-cincinnati",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Applying Word2Vec\n",
    "***\n",
    "The previous state-of-the-art NLP models made heavy use of Word2Vec embeddings:\n",
    "1. Learn a word embedding on a large amount of text (unsupervised)\n",
    "    * It's also possible to train on task-specific text only\n",
    "2. Translate all words in an input sequence to their vectors\n",
    "3. Apply sequences of word vectors to a network model of your choice\n",
    "4. ???\n",
    "5. Profit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "perceived-forward",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "%pip install gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "egyptian-master",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Getting pre-trained vectors\n",
    "***\n",
    "`gensim` is a popular framework for training Word2Vec models. It also has a model zoo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wound-poster",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "import gensim.downloader\n",
    "print(list(gensim.downloader.info()['models'].keys()))\n",
    "# random pick:\n",
    "w2v = gensim.downloader.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "suffering-sauce",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Here is the famous `king - man + woman = queen` example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "little-signature",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "w2v.most_similar(positive=[\"woman\", \"king\"], negative=[\"man\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "criminal-composite",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Data preprocessing\n",
    "***\n",
    "For starters, we do the simplest possible tokenization: Splitting by `\" \"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cooked-fruit",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def tokenize_w2v(w2v, dataset, sentence_function=None):\n",
    "    '''\n",
    "        sentence_function will be applied to each list of translated vectors.\n",
    "        This allows to save a lot of RAM when wishing to aggregate the paragraphs.\n",
    "    '''\n",
    "    x = []\n",
    "    y = np.array(dataset[\"label\"], \"int32\")\n",
    "    for text in tqdm(dataset[\"text\"]):\n",
    "        paragraph = [w2v[token] for token in text.split(\" \") if token in w2v]\n",
    "        if sentence_function is None:\n",
    "            paragraph = np.array(paragraph, \"float32\")\n",
    "        else: \n",
    "            paragraph = sentence_function(paragraph)\n",
    "        x.append(paragraph)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "compact-pottery",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Getting rid of variable length data\n",
    "***\n",
    "For a first simple prototype, we simply sum up all word vectors of a review\n",
    "* Per review, we will get a single vector\n",
    "    * But is that a good approach?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "confidential-channel",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "sum_of_words = lambda vectors: np.sum(vectors, axis=0)\n",
    "x_train, y_train = tokenize_w2v(w2v, raw_datasets[\"train\"], sum_of_words)\n",
    "x_train = np.array(x_train)\n",
    "x_test, y_test = tokenize_w2v(w2v, raw_datasets[\"test\"],sum_of_words)\n",
    "x_test = np.array(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "elect-passion",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# shuffle training samples\n",
    "indices = np.arange(len(x_train))\n",
    "np.random.shuffle(indices)\n",
    "x_train = x_train[indices]\n",
    "y_train = y_train[indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "front-classification",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Training with summed word vectors\n",
    "***\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lesbian-raising",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import (\n",
    "    Input, Dense, Dropout,\n",
    ")\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "input_layer = Input(w2v.vector_size)\n",
    "l = input_layer\n",
    "l = Dense(128, \"relu\")(l)\n",
    "l = Dropout(0.4)(l)\n",
    "l = Dense(1, \"sigmoid\")(l)\n",
    "model = Model(input_layer, l)\n",
    "model.summary(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compact-bridal",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "opt = keras.optimizers.Adam(lr=0.01)\n",
    "model.compile(opt, \"binary_crossentropy\", [\"accuracy\"])\n",
    "model.fit(x_train, y_train, batch_size=128, epochs=1000,\n",
    "          verbose=2, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hybrid-accused",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Evaluation code for W2V based models\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "established-breakdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "loss, acc = model.evaluate(x_test, y_test, batch_size=128)\n",
    "print(f\"Reached {acc:.3f} accuracy and a loss of {loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "printable-strand",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "rise": {
   "auto_select": "none",
   "enable_chalkboard": true,
   "overlay": "<div class='myheader'><img src='img/ai_camp.png' class='ifis_small'></div><div class='ifis_large'><img src='img/ifis_large.png'></div>",
   "scroll": true,
   "start_slideshow_at": "selected"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
